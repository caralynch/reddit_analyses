{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/cara/Documents/reddit_analysis_code')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from reddit_dataclass import RedditData as reddit\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_datasets = pickle.load(open('sentiment_5_datasets.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_cols = ['thread_id', 'id', 'timestamp','author','domain','parent',\n",
    "       'score', 'subject_sentiment_score', 'body_sentiment_score', 'date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for key in sentiment_datasets:\n",
    "    sentiment_datasets[key].data['parent'] = sentiment_datasets[key].data['parent'].str[3:]\n",
    "pickle.dump(sentiment_datasets, open('sentiment_datasets.p', 'wb'))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_datasets = {}\n",
    "\n",
    "for key in sentiment_datasets:\n",
    "    print(f'### WORKING ON {key} ###')\n",
    "\n",
    "    sentiment_data = sentiment_datasets[key].data\n",
    "    thread_data = sentiment_data[thread_cols]\n",
    "    thread_data['level'] = -1\n",
    "\n",
    "    parent_post_id = sentiment_data[sentiment_data.thread_id == sentiment_data.id].id\n",
    "\n",
    "    thread_data.loc[thread_data.id.isin(parent_post_id), 'level'] = 0\n",
    "\n",
    "    children = {}\n",
    "\n",
    "    children[0] = sentiment_data[sentiment_data.parent == sentiment_data.thread_id]\n",
    "    children[0]['parent_comment'] = children[0].id\n",
    "    children[0]['level'] = 1\n",
    "\n",
    "    i = 0\n",
    "    while len(children[i]) > 0:\n",
    "\n",
    "        print(f\"level {i}  number of children: {len(children[i])}\")\n",
    "        i += 1\n",
    "        children[i] = sentiment_data[sentiment_data.parent.isin(children[i-1].id)]\n",
    "        children[i]['level'] = i + 1\n",
    "        parent_mapper = dict(zip(\n",
    "            children[i-1].id, children[i-1].parent_comment\n",
    "        ))\n",
    "        children[i][\"parent_comment\"] = children[i].parent.map(parent_mapper)\n",
    "    \n",
    "    thread_data['parent_comment'] = np.nan\n",
    "\n",
    "    for lvl in children:\n",
    "        thread_data.loc[\n",
    "            thread_data.id.isin(children[lvl].id), \n",
    "            'level'] = lvl + 1\n",
    "        if lvl == 0:\n",
    "            thread_data.loc[\n",
    "                thread_data.id.isin(children[lvl].id),\n",
    "                'parent_comment'] = thread_data[\n",
    "                    thread_data.id.isin(children[lvl].id)].id\n",
    "        else:\n",
    "            parent_mapper = dict(zip(\n",
    "                children[lvl].id, children[lvl].parent_comment\n",
    "            ))\n",
    "            thread_data.loc[\n",
    "                thread_data.id.isin(children[lvl].id),\n",
    "                'parent_comment'] = thread_data[\n",
    "                    thread_data.id.isin(children[lvl].id)].id.map(\n",
    "                        parent_mapper)\n",
    "    \n",
    "    thread_datasets[key] = thread_data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(thread_datasets, open('thread_5_data.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unclassified = {}\n",
    "for key in thread_datasets:\n",
    "    unclassified[key] = thread_datasets[key][thread_datasets[key].level == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subreddit  sentiment  thread_entries     unclassified\n",
      "books 105291  105291 13169\n",
      "conspiracy 423958  423958 46573\n",
      "crypto 444397  444397 36908\n",
      "thedonald 1543  1543 233\n",
      "politics 6428330  6428330 2788540\n"
     ]
    }
   ],
   "source": [
    "print('subreddit  sentiment  thread_entries     unclassified')\n",
    "for key in thread_datasets:\n",
    "    print(f\"{key} {len(sentiment_datasets[key].data)}  {len(thread_datasets[key])} {len(unclassified[key])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if spread over a range of dates\n",
    "for key in unclassified:\n",
    "    date_str = \"\"\n",
    "    for date in unclassified[key].date.unique():\n",
    "        date_str += f\"{date}    \"\n",
    "    print(f\"## {key} ##\")\n",
    "    print(date_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_parent_activity = {}\n",
    "for key in sentiment_datasets:\n",
    "    removed_parent_activity[key] = unclassified[key][unclassified[key].parent.isin(sentiment_datasets[key].removed.id)]\n",
    "    print(f\"{key}   {len(removed_parent_activity[key])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save thread data and unclassified data\n",
    "clean_thread_data = {}\n",
    "for key in thread_datasets:\n",
    "    clean_thread_data[key] = thread_datasets[key][thread_datasets[key].level != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(clean_thread_data, open('clean_5_thread_data.p', 'wb'))\n",
    "pickle.dump(unclassified, open('unclassified_5_thread_data.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_thread_data = pickle.load(open('clean_thread_data.p', 'rb'))\n",
    "unclassified = pickle.load(open('unclassified_thread_data.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "books\n",
      "to remove: 0\n",
      "clean: 92122\n",
      "conspiracy\n",
      "to remove: 0\n",
      "clean: 377385\n",
      "crypto\n",
      "to remove: 0\n",
      "clean: 407489\n",
      "thedonald\n",
      "to remove: 0\n",
      "clean: 1310\n",
      "politics\n",
      "to remove: 0\n",
      "clean: 3639790\n"
     ]
    }
   ],
   "source": [
    "# BUT also need to remove threads that have top comment but no post\n",
    "for key in clean_thread_data:\n",
    "    to_remove = clean_thread_data[key][~\n",
    "            clean_thread_data[key].thread_id.isin(\n",
    "                clean_thread_data[key].id)]\n",
    "    unclassified[key] = pd.concat(\n",
    "        (unclassified[key],\n",
    "        to_remove))\n",
    "    clean_thread_data[key] = clean_thread_data[key][\n",
    "        clean_thread_data[key].thread_id.isin(clean_thread_data[key].id)\n",
    "        ]\n",
    "    print(key)\n",
    "    print(f\"to remove: {len(to_remove)}\")\n",
    "    print(f\"clean: {len(clean_thread_data[key])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unclassified['thedonald']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "pickle.dump(clean_thread_data, open('clean_5_thread_data.p', 'wb'))\n",
    "pickle.dump(unclassified, open('unclassified_5_thread_data.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redditenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 | packaged by conda-forge | (main, Feb  2 2023, 20:20:04) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c482d4c0e8cbaeb0f1b549c3b7cbe4711c34d9cf6d26761b0881cc15f09eabb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
